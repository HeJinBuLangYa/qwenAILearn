AlextNet2012  
1.一举将网络做到8层，证明了大规模神经网络的可能性。  
2.使用了relu激活函数，使得收敛提高了6倍左右。  
3.使用了dropout抑制过拟合的问题。  
4.使用3个3 * 3的卷积层来代替7 * 7的卷积层。  

---
加微信可以聊一聊：
![我的微信](https://www.qingshanzaixian.cn/res/static/img/weixing.jpg "我的微信")

###网络结构图
![AlexNet结构图](https://andreaprovino.it/wp-content/uploads/2020/02/alexnet-architecture-deep-learning-engineer-italia-cnn-network-example-architecture-diagram.png "AlexNet")

